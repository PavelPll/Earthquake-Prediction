{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the time (single vlue) remaining before laboratory earthquakes occur \n",
    "#from real-time seismic data (150000 values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#read data\n",
    "float_data = pd.read_csv(\"train.csv\", #nrows=2e100, \n",
    "                         dtype={\"acoustic_data\": np.float32, \n",
    "                                \"time_to_failure\": np.float32})\n",
    "float_data = float_data.values #np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two functions to create augmenters\n",
    "\n",
    "def running_mean(x, N=3):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    result=(cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "    result=np.insert(result, 0, x[0])\n",
    "    result=np.append(result,x[len(x)-1])\n",
    "    return np.matrix.round(result,0)\n",
    "\n",
    "#frequency filter\n",
    "def fourier(x):\n",
    "    y1=np.fft.fft(x)\n",
    "    l=len(y1)\n",
    "    freq = np.fft.fftfreq(len(y1))\n",
    "    \n",
    "    mean=np.mean(np.abs(y1))\n",
    "    seuil=mean+np.std(np.abs(y1)-mean)\n",
    "\n",
    "    obrez=np.floor((3/5)*l/2)\n",
    "    #print(\"obrez\", obrez)\n",
    "    for i in range(len(y1)):\n",
    "    #if (i<np.floor(l*0.1))|(i>np.floor(0.8*l)):\n",
    "        if ((i>obrez) and i<(l-obrez)):\n",
    "            #if np.abs(y1[i])<seuil:\n",
    "                y1[i]=0\n",
    "    #plt.plot(np.abs(y1))   \n",
    "    #inverse fft to recostruct the signal\n",
    "    yi=np.fft.ifft(y1)\n",
    "    return np.matrix.round(np.real(yi),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b1b5f3277c0359cd1edb4109ac6e1edbcc207e2f"
   },
   "outputs": [],
   "source": [
    "#the idea: convert 150000 values into features to decrease the number of values for RNN input\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import moment, kurtosis, skew\n",
    "from tsfresh.feature_extraction import feature_calculators as ts\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from random import randint\n",
    "\n",
    "def extract_features(z):\n",
    "    #print(z.shape)\n",
    "    #z = z + np.random.normal(0, 0.5, [z.shape[0],z.shape[1]])\n",
    "    #b = normaltest(z, axis=1)\n",
    "    m3 = np.cbrt( moment(z, 3, axis=1) )\n",
    "    \n",
    "    #m21 = autocorr1(z,[1])\n",
    "    #print(\"m21check \", m21>0)\n",
    "    #print(\"mean \",z.mean(axis=1).shape)\n",
    "    #print(\"m21 \",m21.shape)\n",
    "    return np.c_[z.mean(axis=1), \n",
    "                  np.median(np.abs(z), axis=1),\n",
    "                  z.std(axis=1), \n",
    "                  z.max(axis=1),\n",
    "                  z.min(axis=1),\n",
    "                  #kurtosis(z, axis=1),\n",
    "                  -skew(z, axis=1),\n",
    "                  np.quantile(np.abs(z), 0.25, axis=1),\n",
    "                  np.quantile(np.abs(z), 0.75, axis=1),\n",
    "                  #1-np.quantile(z, 0.75, axis=1),\n",
    "                  #b[1],\n",
    "                  -m3,\n",
    "                  #m21,\n",
    "                  #z.shape[1]\n",
    "                ]\n",
    "\n",
    "# For a given ending position \"last_index\", we split the last 150'000 values of \"x\" into 150 pieces of length 1000 each.\n",
    "# From each piece, 34 features are extracted. This results in a feature matrix of dimension (150 time steps x 34 features). \n",
    "def create_X(x, last_index=None, n_steps=150, step_length=1000, aug=0):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "       \n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping\n",
    "    per=x[(last_index - n_steps * step_length):last_index]\n",
    "\n",
    "    #for data augmentation\n",
    "    if aug==1:\n",
    "        flag=randint(0, 2)\n",
    "        if flag==0:\n",
    "            s=np.random.normal(0, 1, per.shape[0])\n",
    "            s=np.matrix.round(s,0)\n",
    "            per=per+s\n",
    "        if flag==1:\n",
    "            per=running_mean(per)\n",
    "        if flag==2:\n",
    "            per=fourier(per)\n",
    "            #print(per)\n",
    "\n",
    "    temp = (per.reshape(n_steps, -1) - 5 ) / 3\n",
    "    \n",
    "    #ac1=np.zeros(150)\n",
    "    ac2=np.zeros(150)\n",
    "    ac3=np.zeros(150)\n",
    "    #c3_1=np.zeros(150)\n",
    "    c3_2=np.zeros(150)\n",
    "    c3_3=np.zeros(150)\n",
    "    mac=np.zeros(150)\n",
    "    mc=np.zeros(150)\n",
    "    for i in range(150):\n",
    "        #ac1[i]=ts.autocorrelation(temp[i,:],1)\n",
    "        ac2[i]=ts.autocorrelation(temp[i,:],2)\n",
    "        ac3[i]=ts.autocorrelation(temp[i,:],3)\n",
    "        #c3_1[i]=ts.c3(temp[i,:],1)/500\n",
    "        c3_2[i]=ts.c3(temp[i,:],2)/500\n",
    "        c3_3[i]=ts.c3(temp[i,:],3)/500\n",
    "        mac[i]=ts.mean_abs_change(temp[i,:])\n",
    "        mc[i]=ts.mean_change(temp[i,:])\n",
    "        \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, 827:]),\n",
    "                 extract_features(temp[:, 970:]),\n",
    "                 #ac1,\n",
    "                 ac2,\n",
    "                 ac3,\n",
    "                 #c3_1,\n",
    "                 c3_2,\n",
    "                 c3_3,\n",
    "                 mac,\n",
    "                 mc,\n",
    "                 temp[:, -1:]]\n",
    "\n",
    "# We call \"extract_features\" three times, so the total number of features is 9 * 3 + 7 (last value) = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "bfe5d35ea534073c07fb63fceb6cf0b06ffd2bd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features=  34\n"
     ]
    }
   ],
   "source": [
    "#generate input for RNN for real data + augmentation\n",
    "n_features = create_X(float_data[0:150000,0], n_steps=150, step_length=1000).shape[1] \n",
    "print(\"n_features= \",n_features)\n",
    "    \n",
    "# The generator randomly selects \"batch_size\" ending positions of sub-time series. For each ending position,\n",
    "# the \"time_to_failure\" serves as target, while the features are created by the function \"create_X\".\n",
    "#Non-random selection gives worse result\n",
    "def generator(data, min_index=0, max_index=None, batch_size=32, n_steps=150, step_length=1000, val=0):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "     \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        if val==0:\n",
    "            #the first half is real data\n",
    "            #the second half is related to augmented data\n",
    "            batch_size1=int(batch_size/2)\n",
    "        else:\n",
    "            # no augmentation for data validation\n",
    "            batch_size1=batch_size\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size1)\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        delta=len(rows)\n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row, 1]\n",
    "            #add data augmentation\n",
    "            if val==0:\n",
    "                samples[j+delta] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length, aug=1)\n",
    "                targets[j+delta] = data[row, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to provide the same input for all epochs based on single random samling of the segments with 150000 length\n",
    "#It is achieved by adding new generator \n",
    "#This part was removed because it does not allow to improve the result\n",
    "\n",
    "# N_STEPS=150\n",
    "# STEP_LENGTH=1000\n",
    "# DEL=5.8\n",
    "# print(\"last: \",int(round(STEP_LENGTH/DEL)))\n",
    "# print(\"last: \",int(round(STEP_LENGTH/(DEL*DEL))))\n",
    "# #define number of batches per epoch\n",
    "# SPE=1000\n",
    "\n",
    "# batch_size = 64\n",
    "# #for 50% augmentation\n",
    "# batch_size1=int(batch_size/2)\n",
    "\n",
    "# min_index=0\n",
    "# max_index = int(len(float_data) - 1)\n",
    "\n",
    "# np.random.seed(seed=1)\n",
    "# arr_rows=[]\n",
    "# for i in range(SPE):\n",
    "#     rows = np.random.randint(min_index + N_STEPS * STEP_LENGTH, max_index, size=batch_size1)\n",
    "#     arr_rows.append(rows)\n",
    "    \n",
    "# def gf(min_index, n_steps, step_length, batch_size1):\n",
    "#     while True:\n",
    "#         np.random.seed(seed=1)\n",
    "#         i=0\n",
    "#         while i<SPE*(1):\n",
    "#             #rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size1)\n",
    "#             rows=arr_rows[i]\n",
    "#             yield rows \n",
    "#             i=i+1\n",
    "# gen = gf(min_index, N_STEPS, STEP_LENGTH, int(batch_size/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d5d2de23fb3a5de57d8447758149ec47fe8e5359"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_gen = generator(float_data, batch_size=batch_size, val=0)\n",
    "#remove augmentation for validation\n",
    "valid_gen = generator(float_data, batch_size=batch_size, val=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "e889e7ab358f5ddf306b8756dfdd1713c71468e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU, Dropout, GRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "print(backend.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "27b40141187a2c2cd7f41c82d85ad1c2524a83f5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 68)                21012     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                1035      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 22,063\n",
      "Trainable params: 22,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "cb = ModelCheckpoint(\"model.hdf5\", monitor='val_loss', save_weights_only=False, period=1)\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(GRU(100, return_sequences=True, input_shape=(None, n_features)))\n",
    "model.add(GRU(68, input_shape=(None, n_features)))\n",
    "#model.add(GRU(21))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define learning rate\n",
    "from os import rename\n",
    "from os.path import isfile\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    #arr=np.array([0.1e-5, 0.1e-4, 0.1e-3, 0.1e-3])\n",
    "    #lr=arr[epoch]\n",
    "#     if epoch<0:\n",
    "#         lr=0.05e-4\n",
    "#     else:\n",
    "#         lr = 0.5e-04\n",
    "    lr=0.0005\n",
    "    print('Learning rate: ', lr)\n",
    "    \n",
    "    if isfile(\"model.hdf5\"):\n",
    "        n=\"model_noise9_ep\"+str(epoch)+\".hdf5\"\n",
    "        rename(\"model.hdf5\",n)\n",
    "        print(\"renamed to \",n)\n",
    "    else:\n",
    "        print(\"no file to rename\")\n",
    "        \n",
    "    return lr\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "aa66e3e5fef5bf7ed4c54538ff67364a277785d3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate:  0.0005\n",
      "no file to rename\n",
      " - 3719s - loss: 2.3104 - val_loss: 2.0933\n",
      "Epoch 2/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep1.hdf5\n",
      " - 3698s - loss: 2.1140 - val_loss: 2.1385\n",
      "Epoch 3/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep2.hdf5\n",
      " - 3703s - loss: 2.1066 - val_loss: 2.0510\n",
      "Epoch 4/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep3.hdf5\n",
      " - 3692s - loss: 2.0951 - val_loss: 2.0953\n",
      "Epoch 5/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep4.hdf5\n",
      " - 3666s - loss: 2.0611 - val_loss: 2.0748\n",
      "Epoch 6/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep5.hdf5\n",
      " - 3662s - loss: 2.0818 - val_loss: 2.0122\n",
      "Epoch 7/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep6.hdf5\n",
      " - 3662s - loss: 2.0667 - val_loss: 2.0229\n",
      "Epoch 8/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep7.hdf5\n",
      " - 3669s - loss: 2.0449 - val_loss: 2.0529\n",
      "Epoch 9/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep8.hdf5\n",
      " - 3669s - loss: 2.0532 - val_loss: 2.0819\n",
      "Epoch 10/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep9.hdf5\n",
      " - 3683s - loss: 2.0333 - val_loss: 2.0783\n",
      "Epoch 11/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep10.hdf5\n",
      " - 3672s - loss: 2.0347 - val_loss: 1.9895\n",
      "Epoch 12/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep11.hdf5\n",
      " - 3662s - loss: 2.0252 - val_loss: 2.0525\n",
      "Epoch 13/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep12.hdf5\n",
      " - 3675s - loss: 2.0330 - val_loss: 2.0463\n",
      "Epoch 14/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep13.hdf5\n",
      " - 3677s - loss: 2.0341 - val_loss: 2.0525\n",
      "Epoch 15/200\n",
      "Learning rate:  0.0005\n",
      "renamed to  model_noise9_ep14.hdf5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e2f877dca26b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                               \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                               callbacks=[cb, lr_scheduler])#n_valid // batch_size)\n\u001b[0m",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program_Files\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,#n_train // batch_size,\n",
    "                              epochs=200,\n",
    "                              verbose=2,\n",
    "                              #callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=100,\n",
    "                              callbacks=[cb, lr_scheduler])#n_valid // batch_size)\n",
    "#val_loss calculation is based on random sampling (32*1000 pieces of 150000 consecutive values from 6e6 values)\n",
    "#this is the way to evaluate the model on the whole dataset\n",
    "#overfitting is compensated by low number of parameters (22063<<150000) in the model \n",
    "#and by data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating submission\n",
    "from keras.models import load_model\n",
    "bestModel = load_model('model_noise9_ep14.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "539d4ee72c944abbfbb377d715f2e6a4b7a0ff0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2624/2624 [03:38<00:00, 12.00it/s]\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "from tqdm import tqdm\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = bestModel.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission_noise9_ep14.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
